---
title: "Proyecto final Fundamentos de Analítica I"
author: "Lizeth Llanos y Diego Agudelo"
date: "12/8/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)

```
El objetivo de este trabajo es implementar los diferentes modelos con el fin de caracterizar y predecir la condición de las páginas web identificadas como benignas o malignas

```{r packages , results='hide', message=F, warning=F}
library(reshape2)
library(ggplot2)
library(caret)
library(MASS)
library(naivebayes)
library(dplyr)
library(stringr)
library(XML)
library(RCurl)
library(MLeval)
library(rpart)
library(rpart.plot)

options(warn=-1)
`%notin%` <- Negate(`%in%`)

```

```{r load data}
# Lectura de datos --------------------------------------------------------
setwd("C:/Users/lllanos/Dropbox/ICESI/Semestre I/Fundamentos de analítica I/Proyecto final")
data = read.csv("PF-02-SitiosMalignos.csv", stringsAsFactors = FALSE)

names(data) = tolower(names(data))

```

## 1. Limpieza de datos

Para iniciar los análisis procedemos a realizar una limpieza a los datos que serán utilizados en el modelo. A continuación se describe paso a paso los procesos realizados.

### 1.1 Baseline

El baseline de los datos es la condición de benigno con un 87.87% 
```{r baseline, results=TRUE, echo=TRUE}
# Baseline
prop.table(table(data$tipo))*100

```

### 1.2 Duplicados 

Se verifica la cantidad de registros y las variables del dataset, contamos con 1781 páginas web y 20 variables. También se verifica que no hayan registros duplicados y el tipo de datos que se tiene en la base de datos.

```{r clean 1, echo=TRUE}
# Limpieza de datos -------------------------------------------------------
dim(data)
sum(duplicated(data))
str(data)

```

### 1.3 Recodificación de variables y eliminación de variables

Con el fin de poder incluir las variables categóricas en los diferentes modelos se realiza una recodificación a la variable: País, Codificación y OS

```{r recode, echo=TRUE}
# Recodificación a la variable codificación
data$codificacion[data$codificacion %in% c("ISO-8859","iso-8859-1","ISO-8859-1","windows-1251","windows-1252")] <- "iso"
data$codificacion[data$codificacion %in% c("UTF-8","utf-8")] <- "utf-8"
data$codificacion[data$codificacion %in% c("None")] <- "none"

# Recodificación a la variable OS

data$os[grep("apache",data$os,ignore.case =T,fixed=F )] <- "apache"
data$os[grep("ats",data$os,ignore.case =T,fixed=F )] <- "apache"
data$os[grep("nginx",data$os,ignore.case =T,fixed=F )] <- "nginx"
data$os[grep("None",data$os,ignore.case =T,fixed=F )] <- "none"
data$os[grep("MICROSOFT-HTTPAPI",data$os,ignore.case =T,fixed=F )] <- "microsoft"
data$os[grep("Microsoft-IIS",data$os,ignore.case =T,fixed=F )] <- "microsoft"

data$os[ data$os %notin% c("apache", "nginx","none","microsoft") ] <- "none"

# Recodificación a la variable País

url_main="https://laendercode.net/es/2-letter-list.html"
theurl=getURL(url_main,.opts = list(ssl.verifypeer = FALSE))
Sys.sleep(0.2)

tables=readHTMLTable(theurl)[[1]][-1,]

data$pais <- toupper(data$pais)
data$pais[tolower(data$pais) %notin% tolower(tables$`ISO 3166 ALPHA-2`)]<-"none"
data$pais[data$pais %in% names(table(data$pais))[table(data$pais)<=100]] <- "Other" 

```

En este caso el procedimiento a seguir fue unificar aquellos valores de las variables que tenían una escritura diferente, por ejemplo: mayúsculas, espacios, carácteres especiales. En el caso de la variable País, se recodificó con el Código ISO y se agruparon como Otros aquellos países que tenían menos de 100 registros. Para la variable OS, se tuvo en cuenta el servidor web de tal forma que se identificaron 3 de estos: Apache, Microsoft y Nginx.

Las variables que no serán utilizadas en ninguno de los análisis será el ID (URL) y Estado, dado que esta variable presenta muchas categorías.

De acuerdo a las características del modelo a ajustar se ajustarán los tipos de las variables, aquellas categóricas se establecen como factor y en el caso de Naive Bayes las númericas se convierten a double.

### 1.4 Verificación de datos faltantes

Se identifican 2 variables que contienen datos faltantes: El tamaño del encabezado, con 812 datos faltantes  y Número de paquetes DNS con 1 dato faltante. 

```{r missing data}
# Verificar datosNA

# table(data$pais)
data$os = as.factor(data$os)
data$codificacion = as.factor(data$codificacion)
data$pais = as.factor(data$pais)

summary(data$largoheader)
summary(data$numpaquetesdns)

data$largoheadermissing = as.factor(ifelse(is.na(data$largoheader), 1, 0))
data$largoheader = ifelse(is.na(data$largoheader), 0, data$largoheader)

data$tipo = factor(data$tipo)
summary(data) #La variable LargoHeader es la que más datos faltantes tiene

```
Dado que largoHeader es la que presenta la mayor cantidad de datos faltantes se procede a crear una nueva variable que identifica si el dato es faltante o no, y posteriormente se reemplazan los NA en por el valor de 0.

En el caso de la variable  Número de paquetes DNS, se decide eliminar el registro que tiene dato faltante de la base de datos.

### 1.5 Análisis exploratorio de los datos

#### 1.5.1 Datos anómalos
Para la identificación de registros anómalos se filtran las variables númericas y se procede a realizar un análisis gráfico mediante boxplot e histogramas.

```{r outliers}
data_num = data[,which(sapply(data,class) %notin% c("character", "factor"))]
data_m = melt(data_num)

ggplot(data_m, aes(variable, value))+geom_boxplot() + facet_wrap(~variable, scales = "free")
ggplot(data_m, aes(value))+geom_histogram() + facet_wrap(~variable, scales = "free")

# Análisis de datos atípicos


atipicos = function(x,p,r){
  qnt<-mean(x, na.rm = T)
 H <-p * sd(x, na.rm = T)
 x2<-ifelse(x> qnt+H | x< qnt-H ,1,0)
 
 if(r==1){
   pos = which(x2==T)
    return(pos)
 }else{
    return(sum(x2))
 }

}

```
En los gráficos boxplot e histogramas se identifican varios datos atípicos, los cuales presentan valores muy superiores a los registrados en las demás observaciones. Se deciden  que una observación es atípica cuando esta es superior o inferior a 5 desviaciones estándar del promedio de cada variable.

```{r rem outliers}
atip = table(unlist(sapply(data_num, atipicos,5,1)))
print("Atípicos identificados")
atip
pos_atip = as.numeric(names(atip)[which(atip>=5)])

print("Atípicos a eliminar")
pos_atip
#Remove atipicos
data[pos_atip,] = NA
data = na.omit(data)


data_num = data[,which(sapply(data,class) %notin% c("character", "factor"))]
data_m = melt(data_num)

ggplot(data_m, aes(variable, value))+geom_boxplot() + facet_wrap(~variable, scales = "free")
ggplot(data_m, aes(value))+geom_histogram() + facet_wrap(~variable, scales = "free")
```

Finalmente se identifican 5 registros que presentan valores atípicos en la mayoría de las variables y se procede a eliminarlos de la base de datos. 


#### 1.5.2 Análisis gráfico y descriptivo

Con el fin de explorar el comportamiento de algunas variables se realizarán algunos análisis gráficos y descriptivos en función de la variable que identifica como maligna o benigna una página web.

```{r plots}
ggplot(data, aes(numeroips))+ geom_density(aes(fill=tipo2),alpha=0.4)+labs(fill="Tipo")

ggplot(data, aes(largourl))+ geom_density(aes(fill=tipo2),alpha=0.4) +labs(fill="Tipo")

ggplot(data, aes(numpaquetesdns))+ geom_density(aes(fill=tipo2),alpha=0.4)+labs(fill="Tipo") 

ggplot(data, aes(codificacion))+ geom_bar(aes(fill=tipo2))+labs(fill="Tipo")
ggplot(data, aes(os))+ geom_bar(aes(fill=tipo2))+labs(fill="Tipo")
ggplot(data, aes(pais))+ geom_bar(aes(fill=tipo2))+labs(fill="Tipo")

#data %>% group_by(tipo) %>% #summarise(mean(numbytes),mean(numpaquetesdns),mean(numpaquetesenviados))
```



## 2. Ajuste de modelos predictivos

### 2.1 Protocolo de evaluación y calibración

Para el ajuste de los diferentes modelos se esteblece un 80% de los datos para entrenamiento y un 20% para el test, en la parte del entrenamiento se utilizará el método de k-fold con k=5 para la selección de los hiperparámetros.

```{r train test}
# [1] "url"                  "largourl"             "numcarespeciales"    
#  [4] "codificacion"         "os"                   "largoheader"         
#  [7] "pais"                 "estado"               "numpaquetestcp"      
# [10] "puertosremotos"       "numeroips"            "numbytes"            
# [13] "numpaquetesenviados"  "numpaquetesrecibidos" "numbytesenviados"    
# [16] "numbytesrecibidos"    "numpaquetesgenerados" "numpaquetesdns"      
# [19] "tipo"                 "tipo2"                "largoheadermissing" 


```

### 2.2 Regresión logística

Se remueve la variable codificación
```{r logit, results=FALSE}
data_model = na.omit(data[, -c(1,4,8,20)])

set.seed(500) 
trainIndex <- createDataPartition(data_model$tipo, p = .8, list = FALSE, times = 1)
length(trainIndex)

webTrain <- data_model[ trainIndex,]
webTest <-  data_model[-trainIndex,]

model_logreg1 <- train(tipo~., webTrain, 
                       method="glm", family="binomial",
                       trControl=trainControl(method="cv", number=5),
                       preProcess=c("center", "scale"))

model_logreg1 




trainControl=trainControl(method="cv", number=5)
set.seed(123) 
model_stepBoth <- train(tipo ~., data = data_model,
                        method = "glmStepAIC", direction ="forward", 
                        trControl = trainControl,
                        preProcess=c("center", "scale"),
                        trace=TRUE)




```

```{r logit confusion matrix}
model_stepBoth$finalModel

predictions_test<-predict(object=model_stepBoth, webTest)
confusionMatrix(predictions_test, factor(webTest$tipo))

predictions<-predict(object=model_stepBoth, webTrain)
confusionMatrix(predictions, factor(webTrain$tipo))


```

### 2.3 Naive Bayes

```{r naive}

data_model_naive = na.omit(data[, -c(1,8,20)])

set.seed(500) 
trainIndex <- createDataPartition(data_model_naive$tipo, p = .8, list = FALSE, times = 1)
length(trainIndex)

webTrain <- data_model_naive[ trainIndex,]
webTest <-  data_model_naive[-trainIndex,]

id= which(sapply(data_model_naive,class)!="factor")
data_model_n = mutate_each(data_model_naive, funs(as.double), id)

tgrid1 = expand.grid(laplace=c(10, 7, 6, 5, 4.5, 4, 3.5, 3, 2, 1, 0.1, 0.01, 0.001, 0.0001, 0),
                     usekernel=c(FALSE), adjust=c(0))
tgrid2 = expand.grid(laplace=c(10, 7, 6, 5, 4.5, 4, 3.5, 3, 2, 1, 0.1, 0.01, 0.001, 0.0001, 0),
                     usekernel=c(TRUE), adjust=c(1))
tgrid = rbind(tgrid1, tgrid2)


# data_model %>%                           # Se va a trabajar sobre el dataset completo
#   filter(tipo == "0") %>%      # Sólo vamos a considerar los registros con el "LEAVE" en la variable objetivo
#   select_if(is.numeric) %>%         # Sólo nos interesan las variables numéricas
#   cor() %>%                         # obtenemos la matriz de correlaciones
#   corrplot::corrplot()


webTrainX = webTrain[,-which(names(webTrain)=="tipo")]

set.seed(123)
model_nb5 <- train(webTrainX, webTrain$tipo,
                   method = "naive_bayes",
                   tuneGrid=tgrid,
                   trControl = trainControl)
model_nb5

 
```

```{r naive confusion matrix}
model_nb5$bestTune

predictions_test<-predict(object=model_nb5, webTest)
 confusionMatrix(predictions_test, factor(webTest$tipo))

predictions<-predict(object=model_nb5, webTrain)
 confusionMatrix(predictions, factor(webTrain$tipo))
 
```

### 2.4 Árbol de decisión

```{r tree}

data_model_tree = na.omit(data[, -c(1,8,20)])

set.seed(500) 
trainIndex <- createDataPartition(data_model_tree$tipo, p = .8, list = FALSE, times = 1)
length(trainIndex)

webTrain <- data_model_tree[ trainIndex,]
webTest <-  data_model_tree[-trainIndex,]

trainControl=trainControl(method="cv", number=5)
set.seed(9876)
model_arbol1 <- train(tipo~., webTrain, 
                      method='rpart',
                      trControl = trainControl)
model_arbol1
plot(model_arbol1)
```

```{r tree confusion matrix}
m <- model_arbol1$finalModel
plot(m)
text(m, cex=0.7)

rpart.plot(m)  
m$variable.importance # índice de importancia (no normalizada)
plot(m$variable.importance)
```

### 2.5 Modelo de ensamble: Random Forest


### 2.6 Conclusión

## 3. Cambio de representación y segmentación de los casos

### 3.1 Análisis de componentes principales (ACP)

### 3.2 Clúster k-means

### 3.3 Clúster Jerárquico

### 3.4 Conclusiones
