---
title: "Proyecto final analítica"
author: "Lizeth Llanos y Diego Agudelo"
date: "12/8/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)

```
El objetivo de este trabajo es implementar los diferentes modelos con el fin de caracterizar y predecir la condición de las páginas web identificadas como benignas o malignas

```{r, warning=FALSE, results=FALSE}
library(reshape2)
library(ggplot2)
library(caret)
library(MASS)
library(naivebayes)
library(dplyr)
library(stringr)
library(XML)
library(RCurl)

# Lectura de datos --------------------------------------------------------
setwd("C:/Users/lllanos/Dropbox/ICESI/Semestre I/Fundamentos de analítica I/Proyecto final")
data = read.csv("PF-02-SitiosMalignos.csv", stringsAsFactors = FALSE)

names(data) = tolower(names(data))

`%notin%` <- Negate(`%in%`)

```

## 1. Limpieza de datos

Para iniciar los análisis procedemos a realizar una limpieza a los datos que serán utilizados en el modelo.

### 1.1 Baseline

El baseline de los datos es la condición de benigno con un 87.87% 
```{r, results=TRUE, echo=TRUE}
# Baseline
prop.table(table(data$tipo))*100

```
### 1.2 Duplicados 
Se verifica la cantidad de registros y las variables del dataset, contamos con 1781 páginas web y 20 variables. También se verifica que no hayan registros duplicados

```{r, echo=TRUE}
# Limpieza de datos -------------------------------------------------------
dim(data)
str(data)
sum(duplicated(data))

```

### 1.3 Recodificación de variables y eliminación de variables

Con el fin de poder incluir las variables categóricas a los diferentes modelos se realiza una recodificación a la variable: País, Codificación y OS

```{r}
data$codificacion[data$codificacion %in% c("ISO-8859","iso-8859-1","ISO-8859-1","windows-1251","windows-1252")] <- "iso"
data$codificacion[data$codificacion %in% c("UTF-8","utf-8")] <- "utf-8"
data$codificacion[data$codificacion %in% c("None")] <- "none"

data$os[grep("apache",data$os,ignore.case =T,fixed=F )] <- "apache"
data$os[grep("ats",data$os,ignore.case =T,fixed=F )] <- "apache"
data$os[grep("nginx",data$os,ignore.case =T,fixed=F )] <- "nginx"
data$os[grep("None",data$os,ignore.case =T,fixed=F )] <- "none"
data$os[grep("MICROSOFT-HTTPAPI",data$os,ignore.case =T,fixed=F )] <- "microsoft-httpapi"
data$os[grep("Microsoft-IIS",data$os,ignore.case =T,fixed=F )] <- "microsoft-iis"

data$os[ data$os %notin% c("apache", "nginx","none","microsoft-httpapi","microsoft-iis") ] <- "none"

url_main="https://laendercode.net/es/2-letter-list.html"
theurl=getURL(url_main,.opts = list(ssl.verifypeer = FALSE))
tables=readHTMLTable(theurl)[[1]][-1,]

data$pais <- toupper(data$pais)
data$pais[tolower(data$pais) %notin% tolower(tables$`ISO 3166 ALPHA-2`)]<-"none"
table(data$pais)
```

Las variables que no serán utilizadas en ninguno de los análisis son: URL y Estado

### 1.4 Verificación de datos faltantes

Se identifican 2 variables que contienen datos faltantes: El tamaño del encabezado, con  y Número de paquetes DNS  

```{r}
# Verificar datosNA

# table(data$pais)
data$os = as.factor(data$os)
data$codificacion = as.factor(data$codificacion)
summary(data) #La variable LargoHeader es la que más datos faltantes tiene

data$largoheadermissing = as.character(ifelse(is.na(data$largoheader), 1, 0))
data$largoheader = ifelse(is.na(data$largoheader), 0, data$largoheader)

data$tipo = factor(data$tipo)

data = data[which(is.na(data$numpaquetesdns)),]
```


### 1.5 Análisis exploratorio de los datos

#### 1.5.1 Datos anómalos
Para la identificación de registros anómalos se filtran las variables númericas y se procede a realizar un análisis gráfico mediante boxplot e histogramas

```{r}
data_num = data[,which(sapply(data,class) %notin% c("character", "factor"))]
data_m = melt(data_num)

ggplot(data_m, aes(variable, value))+geom_boxplot() + facet_wrap(~variable, scales = "free")
ggplot(data_m, aes(value))+geom_histogram() + facet_wrap(~variable, scales = "free")

# Análisis de datos atípicos

atipicos = function(x,p){
  qnt<-quantile(x, probs=c(.25, .75), na.rm = T)
 H <-p * IQR(x, na.rm = T)
 x2<-ifelse(x> qnt[2]+H | x< qnt[1]-H ,1,0)
 return(sum(x2))
}

t_atipicos = cbind("Conteo Atipicos" = apply(na.omit(data_num), 2,atipicos,5),"% Atipicos" = apply(na.omit(data_num), 2,atipicos,5)/1781*100)

t_atipicos[order(t_atipicos[,1])]
```

En los gráficos boxplot e histogramas se identifican varios datos atípicos, los cuales presentan valores muy superiores a los registrados en las demás observaciones. Se deciden  otro eliminando aquellas observaciones superiores a 3IQR. De acuerdo a las características del modelo a ajustar se usará el data set correspondiente.

#### 1.5.2 Análisis gráfico y descriptivo

## 2. Ajuste de modelos predictivos

### 2.1 Protocolo de evaluación y calibración

Para el ajuste de los diferentes modelos se esteblece un 75% de los datos para entrenamiento y un 25% para el test, en la parte del entrenamiento se utilizará el método de k-fold con k=5 para la selección de los hiperparámetros.

```{r}
data_model = na.omit(data[, -c(1,7,8,20)])

set.seed(500) 
trainIndex <- createDataPartition(data_model$tipo, p = .75, list = FALSE, times = 1)
length(trainIndex)

webTrain <- data_model[ trainIndex,]
webTest <-  data_model[-trainIndex,]

```

### 2.2 Regresión logística

### 2.3 Naive Bayes

```{r}
# id= which(sapply(data_model,class)!="character")
# data_model_n = mutate_each(data_model, funs(as.double), id)
# 
# tgrid1 = expand.grid(laplace=c(10, 7, 6, 5, 4.5, 4, 3.5, 3, 2, 1, 0.1, 0.01, 0.001, 0.0001, 0), 
#                      usekernel=c(FALSE), adjust=c(0))
# tgrid2 = expand.grid(laplace=c(10, 7, 6, 5, 4.5, 4, 3.5, 3, 2, 1, 0.1, 0.01, 0.001, 0.0001, 0), 
#                      usekernel=c(TRUE), adjust=c(1))
# tgrid = rbind(tgrid1, tgrid2)
# 
# 
# data_model %>%                           # Se va a trabajar sobre el dataset completo
#   filter(tipo == "0") %>%      # Sólo vamos a considerar los registros con el "LEAVE" en la variable objetivo
#   select_if(is.numeric) %>%         # Sólo nos interesan las variables numéricas
#   cor() %>%                         # obtenemos la matriz de correlaciones
#   corrplot::corrplot()    
# 
# 
# webTrainX = webTrain[,-14]
# set.seed(123) 
# model_nb5 <- train(webTrainX, webTrain$tipo,
#                    method = "naive_bayes", 
#                    tuneGrid=tgrid,
#                    trControl = trainControl)
# model_nb5
# model_nb5$bestTune
# 
# predictions<-predict(object=model_nb5, webTest)
# confusionMatrix(predictions, factor(webTest$tipo))
```

### 2.4 Árbol de decisión


### 2.5 Modelo de ensamble: Random Forest


### 2.6 Conclusión

## 3. Cambio de representación y segmentación de los casos

### 3.1 Análisis de componentes principales (ACP)

### 3.2 Clúster k-means

### 3.3 Clúster Jerárquico

### 3.4 Conclusiones
