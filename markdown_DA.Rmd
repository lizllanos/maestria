---
title: "Untitled"
author: "D"
date: "December 10, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
library(reshape2)
library(ggplot2)
library(caret)
library(MASS)
library(naivebayes)
library(dplyr)
library(stringr)
library(XML)
library(RCurl)
library(corrplot)

# Lectura de datos --------------------------------------------------------
setwd("D:/Master_CD/Fundamentos_1/proyecto_final")
data = read.csv("PF-02-SitiosMalignos.csv", stringsAsFactors = FALSE)

# Categorizaci?n de las variables.

attach(data)

'%notin%' <- Negate('%in%')
'%ni%' <- Negate('%in%')

Codificacion[Codificacion %in% c("ISO-8859","iso-8859-1","ISO-8859-1","windows-1251","windows-1252")] <- "iso"
Codificacion[Codificacion %in% c("UTF-8","utf-8")] <- "utf-8"
Codificacion[Codificacion %in% c("None")] <- "none"

OS[grep("apache",OS,ignore.case =T,fixed=F )] <- "apache"
OS[grep("ats",OS,ignore.case =T,fixed=F )] <- "apache"
OS[grep("nginx",OS,ignore.case =T,fixed=F )] <- "nginx"
OS[grep("None",OS,ignore.case =T,fixed=F )] <- "none"
OS[grep("MICROSOFT-HTTPAPI",OS,ignore.case =T,fixed=F )] <- "microsoft-httpapi"
OS[grep("Microsoft-IIS",OS,ignore.case =T,fixed=F )] <- "microsoft-iis"
OS[ OS %ni% c("apache", "nginx","none","microsoft-httpapi","microsoft-iis") ] <- "none"


url_main="https://laendercode.net/es/2-letter-list.html"
theurl=getURL(url_main,.opts = list(ssl.verifypeer = FALSE))
Sys.sleep (0.2) 
tables=readHTMLTable(theurl)[[1]][-1,]

data$Pais <- toupper(data$Pais)
data$Pais[tolower(data$Pais) %ni% tolower(tables$`ISO 3166 ALPHA-2`)]<-"none"
data$Pais[data$Pais %in% names(table(data$Pais))[table(data$Pais)<=10]] <- "other" 

table(data$Pais)
names(data) = tolower(names(data))

data$os = as.factor(data$os)
data$codificacion = as.factor(data$codificacion)
summary(data) #La variable LargoHeader es la que m?s datos faltantes tiene

data$largoheadermissing = as.character(ifelse(is.na(data$largoheader), 1, 0))
data$largoheader = ifelse(is.na(data$largoheader), 0, data$largoheader)

data$tipo = factor(data$tipo)

data = data[-which(is.na(data$numpaquetesdns)),]

data_num = data[,which(sapply(data,class) %notin% c("character", "factor"))]
data_m = melt(data_num)


atipicos = function(x,p,r){
  qnt<-mean(x, na.rm = T)
  H <-p * sd(x, na.rm = T)
  x2<-ifelse(x> qnt+H | x< qnt-H ,1,0)
  
  if(r==1){
    pos = which(x2==T)
    return(pos)
  }else{
    return(sum(x2))
  }
  
}

atip = table(unlist(sapply(data_num, atipicos,4.2,1)))
atip
pos_atip = as.numeric(names(atip)[which(atip>=5)])
pos_atip
#Remove atipicos
data[pos_atip,] = NA
data = na.omit(data)

data_num = data[,which(sapply(data,class) %notin% c("character", "factor"))]
data_m = melt(data_num)


```

## 3.1 Análisis de componentes principales (ACP)

El primer paso para realizar el análisis de componentes principales es verificar que las variables se encuentren correlacionadas, este procedimiento se realizó tanto para los casos malignos como para todos los casos, con el objetivo de comparar los resultados.

```{r correlación}
library("factoextra")
library("FactoMineR")

data_3 <- cbind.data.frame(data_num,tipo=data$tipo) %>% filter(tipo==1) %>% dplyr::select(-tipo) 
data_3_all <- cbind.data.frame(data_num,tipo=data$tipo) %>% dplyr::select(-tipo)  

par(mfrow = c(1, 2))
corMat <- cor(data_3)
corrplot(corMat, type="upper", order="hclust",title= "Malignos",mar=c(0,0,1,0))

corMat_all <- cor(data_3_all)
corrplot(corMat_all, type="upper", order="hclust",title= "Todos los datos",mar=c(0,0,1,0))


```


Del grafico de correlación para los datos malignos se observa una fuerte o moderada correlación positiva entre las variables asociadas a las tranferencias cliente-servidor, adicionalmente las variables largourl y numcarespeciales presentan una moderada o debil correlación negativa con la mayoria de las variables asociadas a las transferencias cliente-servidor, por ejemplo entre más caracteres tenga la url, se espera que en promedio se generen menos paquetes DNS en la comunicación cliente-servidor. En cuanto al grafico con todos los datos se puede observar que al ingresar los datos veningos se pierden las corelaciones negativas observadas para las variables largourl y numcarespeciales, es decir los registros venignos se ven influeciados en gran medida por estas variables.





```{r PCA}

res.pca <- PCA(data_3,ncp=4,  graph = FALSE)
get_eig(res.pca)
fviz_screeplot(res.pca, addlabels = TRUE, ylim = c(0, 80))

```

De los resultados obtenidos se puede observar que para acumular un 90% de la información de los datos originales, es necesario retener 4 componentes principales que acumulan el 91.3% de la variabilidad total observada.

```{r Componentes}

fviz_pca_biplot(res.pca)

```

El grafico de individuos y variables se puede observar que las variables largourl y numcarespeciales se encuentran en dirección opuesta a todas las demás variables,es decir presentan un comportamiento inverso con respecto a los demás, adicionalmente estas variables se encuentran aportando a la construcción de las dos primeras componentes, mientras que las variables asociadas a las transferencias cliente-servidor se encuentran en la misma dirección de la componente 1, por lo tanto la componente 1 va a representar muy bien estas variables, finalmente las variables numpaquetesdns y numeroips son importantes en la construcción de ambas componentes, en cuanto a las páginas web se puede observar que se distinguen 3 grupos diferentes en el primer plano factorial.

```{r}
comp <- data.frame(res.pca$ind$coord[,1:4,drop=F])

load_cor <- t(cor(comp,data_3))
data_plot=melt(load_cor)

ggplot(aes( x = factor(Var1), y=value),data =data_plot ) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  coord_flip()+
  facet_wrap(~ factor(Var2)) +
  ylab("Correlación de pearson") +
  xlab("Variables") +
  theme_bw() 

```

Finalmente se muestran la contribucción de cada una de las variables a las primeras 4 componentes en terminos de la correlación de pearson, donde finalmente se comprueba que la primera componente está compuesta principalmente de las variables asociadas a las transferencias cliente-servidor con correlaciones en su mayoria superiores a 0.6, mientras que la segunda componente se encuentra compuesta principalmente por las variables largourl, numcarespeciales y numbytesenviados, adicionalmente se observa que el puertosremotos está generando en su mayoria el patron de la tercera componente,la cuarta componente está construida principalmente de la variables largoheader. 

## 3.2 Clustering K-means

Inicialmente se grafica el primer plano factorial considerando 1,2,3 y 4 cluster.

```{r kmeans}


set.seed(1234)
kmClustering0 <- kmeans(data_3, 1, nstart=100, iter.max=150)
kmClustering1 <- kmeans(data_3, 3, nstart=100, iter.max=150)
kmClustering2 <- kmeans(data_3, 4, nstart=100, iter.max=150)
kmClustering3 <- kmeans(data_3, 5, nstart=100, iter.max=150)

library(gridExtra)

c0 <- fviz_cluster(kmClustering0, data_3, geom="point")#labelsize = 5)
c1 <- fviz_cluster(kmClustering1, data_3, geom="point")#labelsize = 5)
c2 <- fviz_cluster(kmClustering2, data_3, geom="point")#labelsize = 5)
c3 <- fviz_cluster(kmClustering3, data_3, geom="point")#labelsize = 5)

grid.arrange(c0,c1,c2,c3,ncol=2)


```

De manera visual solo enfocándose en el primer plano factorial el número de clúster ideal que se ajusta mejor al conjunto de datos parece ser el número 3.


### 3.2.1 Determinación del número de clusters K

#### 3.2.1.1 Método del codo

```{r codo}


vars <- apply(data_3,2,var)
sumvars <- sum(vars)
wss <- (nrow(data_3) - 1)*sumvars # Para obtener el TSS multiplicamos la varianza por (N-1)

# Para K>1, calculamos las diferencias cuadradas en un ciclo
set.seed(1234)
maxK <- 10
for (k in 2:maxK) { 

  wssK <- 0 # Aquí va a quedar el total de WSS para el k actual
  kmClustering <- kmeans(data_3, k, nstart=30, iter.max=150)
  data_3$clusters <- kmClustering$cluster
  
  for(i in 1:k) { # Recorrido de los clusters

    clusterData <- subset(data_3, clusters==i)
    centroide <- apply(clusterData, 2, FUN=mean)
    wssK <- wssK + sum(apply(clusterData, 1, FUN=function(fila) {sum((fila-centroide)^2)}))
  }
  # Una vez calculado el wss del K actual, lo guardamos en el vector de WSS
  wss[k] <- wssK
}
wss

plot(1:maxK, wss, type = "b", xlab = "Número de Clusters", ylab = "Within groups sum of squares")  
grid()

```

En este caso tenemos un codo muy evidente donde a partir del clúster número 3 se observan reducciones poco significativas en el WSS, por lo tanto, el clúster sugerido utilizando el método del codo seria el número 3, lo ideal en este punto sería escoger clústers pequeños ya que a medida que crece el número de clúster es más difícil de interpretar estos resultados.



#### 3.2.1.2 Método de Calinski-Harabasz

```{r  Calinski}
library(fpc)

set.seed(11111)
kmClusteringRuns.ch <- kmeansruns(data_3, krange=1:10, criterion="ch")
summary(kmClusteringRuns.ch)

val_ch <- kmClusteringRuns.ch$crit
rm(kmClusteringRuns.ch)

plot(val_ch,type="b",las=1,xlab="K",ylab="Valor del Calinski-Harabasz") 
grid()

```

Encontramos que según el criterio Calinski Harabasz el mejor particionamiento es con k=9, pero como tenemos la restricción de buscar de 3 a 5 clusters, podemos ver que k=4 tiene valor de CH mayor en comparación con 3 y 5 clusters.


#### 3.2.1.3 Método de la silueta

```{r silueata}

library(cluster)

distancias <- dist(data_3)

# Iteramos de K=2 a 8
val_k <- 2:10
val_sil<- 0
for(k in val_k) {
  resultadoKMeansK <- kmeans(data_3, centers = k, nstart =30, iter.max=150)
  sil <- silhouette(resultadoKMeansK$cluster, dist(data_3))
  val_sil[k-1] <- mean(sil[, "sil_width"])
}

rm(distancias, resultadoKMeansK, sil)
gc()

val_sil

plot(val_k, val_sil,las=1,
       type = "b", pch = 19, frame = FALSE, 
       xlab = "K",
       ylab = "Silueta promedio")
grid()



```

Encontramos que, según el criterio de silueta promedio, entre 3 y 5, el mejor particionamiento se logra con K=3 con una silueta promedio de 0.78, ahora veamos los perfiles de las siluetas de los 3 clusterings (con K igual a 3, 4 y 5).


```{r silue}
k <- 3
set.seed(1234)
kmClustering3 <- kmeans(data_3, k, nstart=100, iter.max=150)
silueta <- silhouette(kmClustering3$cluster, dist(data_3))
f1 =fviz_silhouette(silueta, label = FALSE, print.summary = TRUE, main="Silueta para K=3")


k <- 4
set.seed(1234)
kmClustering4 <- kmeans(data_3, k, nstart=100, iter.max=150)
silueta <- silhouette(kmClustering4$cluster, dist(data_3))
f2=fviz_silhouette(silueta, label = FALSE, print.summary = TRUE, main="Silueta para K=4")

k <- 5
set.seed(1234)
kmClustering5 <- kmeans(data_3, k, nstart=100, iter.max=150)
silueta <- silhouette(kmClustering5$cluster, dist(data_3))
f3=fviz_silhouette(silueta, label = FALSE, print.summary = TRUE, main="Silueta para K=5")

grid.arrange(f1,f2,f3,ncol=2)

```

Podemos ver que con K=5 las siluetas de cada cluster son peores que con K=3 o K=4. En estos dos útlimos clusterings, tenemos un gran cluster que posee valores de silueta positivos, y no se presentan sitios web con siluetas negativas.

#### 3.2.1.4 Bootstrap de los cluster

```{r jacar,results=FALSE}
set.seed(3333)
modelo_bootstrap_k3 <- clusterboot(data_3, clustermethod = kmeansCBI, krange=3, B=100, iter.max=150)

modelo_bootstrap_k4 <- clusterboot(data_3, clustermethod = kmeansCBI, krange=4, B=100, iter.max=150)

modelo_bootstrap_k5 <- clusterboot(data_3, clustermethod = kmeansCBI, krange=4, B=100, iter.max=150)

```



```{r, bost}

k_3=cbind.data.frame(mean_jacard=modelo_bootstrap_k3$bootmean #vector de las estabilidades de cada cluster (promedio de Jaccard)
, disolucion=modelo_bootstrap_k3$bootbrd # número de disoluciones de los clusters en los B runs ejecutados (Jaccard < 0.5)
, recuperaciones=modelo_bootstrap_k3$bootrecover )# número de recuperaciones de los clusters en los B runs ejecutados (Jaccard > 0.75)

k_3


```



```{r}
k_4=cbind.data.frame(mean_jacard=modelo_bootstrap_k4$bootmean #vector de las estabilidades de cada cluster (promedio de Jaccard)
, disolucion=modelo_bootstrap_k4$bootbrd # número de disoluciones de los clusters en los B runs ejecutados (Jaccard < 0.5)
, recuperaciones=modelo_bootstrap_k4$bootrecover )# número de recuperaciones de los clusters en los B runs ejecutados (Jaccard > 0.75)

k_4 


```

```{r}
k_5=cbind.data.frame(mean_jacard=modelo_bootstrap_k5$bootmean #vector de las estabilidades de cada cluster (promedio de Jaccard)
, disolucion=modelo_bootstrap_k5$bootbrd # número de disoluciones de los clusters en los B runs ejecutados (Jaccard < 0.5)
, recuperaciones=modelo_bootstrap_k5$bootrecover )# número de recuperaciones de los clusters en los B runs ejecutados (Jaccard > 0.75)

k_5


resumen= cbind.data.frame(c(3,4,5),rbind.data.frame(apply(k_3,2,mean),apply(k_4,2,mean),apply(k_5,2,mean)))
colnames(resumen)=c("k",colnames(k_5))
resumen

```

Del resumen general se puede observar que en promedio el particionamiento k=5 fue el que presentó mejores resultados, clúster estables con promedio de Jaccard en general de  0.87 en promedio, bajas tasas de disolución con un promedio de 10 para todos los clusters y una tasa de recuperación media de 79.

### 3.2.2 Resumen de selección del K

A continuación se muestra un resumen de las metodologías utilizadas para seleccionar el número de particionamientos en los datos.

**Criterio**|**K=3**|**K=4**|**K=5**
:-----:|:-----:|:-----:|:-----:
Método del codo|X| | 
Método de Calinski-Harabasz| |X| 
Método de la silueta|X| | 
Bootstrap de los cluster| | |X

Según el resumen de los criterios de selección del K, K=3 es seleccionado por dos métodos, mientras que k=4 y k=5 son seleccionados solo por un método, por lo tanto, el número de clúster ideal sería K=3, dadas sus mejores métricas de WSS y silueta, aunque K=4 no estaría desacertado dados el Método de Calinski-Harabasz y buenos valores de clusterboot.  


### 3.2.3 Caracterización de los clusters

Con el objetivo de tratar de dar caracteristicas a los clusters encontrados se realizaron graficos de densidad para cada variable particionada por cada clúster.  

```{r caracter}

fviz_cluster(kmClustering1, data_3, geom="point")
cluster_kmeans <- kmClustering1$cluster
data_3$clusters <- as.factor(cluster_kmeans)


 p1 <- ggplot(data_3, aes(x=largourl)) + 
  geom_density(aes(group=clusters, colour=clusters, fill=clusters), alpha=0.1) +
  scale_colour_manual(values=rainbow(3))
  
 p2 <- ggplot(data_3, aes(x=numcarespeciales)) + 
  geom_density(aes(group=clusters, colour=clusters, fill=clusters), alpha=0.1) +
  scale_colour_manual(values=rainbow(3))

 p3 <- ggplot(data_3, aes(x=numeroips)) + 
  geom_density(aes(group=clusters, colour=clusters, fill=clusters), alpha=0.1) +
  scale_colour_manual(values=rainbow(3))
  
 p4 <- ggplot(data_3, aes(x=numpaquetestcp)) + 
  geom_density(aes(group=clusters, colour=clusters, fill=clusters), alpha=0.1) +
  scale_colour_manual(values=rainbow(3))
  
 p5 <- ggplot(data_3, aes(x=numbytes)) + 
  geom_density(aes(group=clusters, colour=clusters, fill=clusters), alpha=0.1) +
  scale_colour_manual(values=rainbow(3))
  
 p6 <- ggplot(data_3, aes(x=numpaquetesenviados)) + 
  geom_density(aes(group=clusters, colour=clusters, fill=clusters), alpha=0.1) +
  scale_colour_manual(values=rainbow(3))
  
 p7 <- ggplot(data_3, aes(x=numbytesrecibidos)) + 
  geom_density(aes(group=clusters, colour=clusters, fill=clusters), alpha=0.1) +
  scale_colour_manual(values=rainbow(3))
  
 p8 <- ggplot(data_3, aes(x=numpaquetesgenerados)) + 
  geom_density(aes(group=clusters, colour=clusters, fill=clusters), alpha=0.1) +
  scale_colour_manual(values=rainbow(3))
  
 p9 <- ggplot(data_3, aes(x=numpaquetesrecibidos)) + 
  geom_density(aes(group=clusters, colour=clusters, fill=clusters), alpha=0.1) +
  scale_colour_manual(values=rainbow(3))

 p10 <- ggplot(data_3, aes(x=numpaquetesdns)) + 
  geom_density(aes(group=clusters, colour=clusters, fill=clusters), alpha=0.1) +
  scale_colour_manual(values=rainbow(3))
  
 grid.arrange(p1,p2,p3,p4,p5,p6,p7,p8,p9,p10,ncol=3)
 

```

Los tres segmentos encontrados tienen comportamientos bien definidos:

* Clúster rojo( 8 páginas web)
    * pocos caracteres en la url 
    * pocos caracteres especiales en url
    * pocos números de parquetes DNS generados en la comunicación 
    * grandes transferencias cliente-servidor
* Clúster verde( 12 páginas web)
    * pocos caracteres en la url 
    * pocos caracteres especiales en url
    * grandes números de parquetes DNS generados en la comunicación 
    * valores medios en las transferencias cliente-servidor
* Cluster azul ( 195 páginas web)
    * pocos caracteres en la url 
    * pocos caracteres especiales en url
    * pocos números de parquetes DNS generados en la comunicación 
    * pocas transferencias cliente-servidor

## 3.3 Cluster Jerarquico (usando Ward).

Ahora se procede a usar un cluster jerarquico con K=3 para ver las diferencias de los resultados con la metodología K-means.

```{r jerarquico}

data_jerar <- data_3 %>% dplyr::select(-clusters) 

distancias <- dist(data_jerar, method = "euclidean")
as.matrix(distancias)[1:6, 1:6]

modelo_ward <- hclust(distancias, method="ward.D2")
modelo_ward

clustersWard <- cutree(modelo_ward, k=3)

comp$clustersWard <- as.factor(clustersWard)
table(clustersWard)

plot(modelo_ward, main="Dendrograma con Ward", ylab="Distancia", xlab = "Instancias", hang=-1)
rect.hclust(modelo_ward, k=3)



```

En el dendograma se puede observar el corte para seleccionar tres particiones en los datos, el ultimo grupo de la derecha presenta un comportamiento un poco atipico en comparación de los demás clusters. 


```{r comparacion}
ggplot(data=data.frame(comp), aes(x=Dim.1, y=Dim.2)) +
geom_point(size=2, alpha=.5, colour=clustersWard) + labs(title="Ward")


comp$cluster_kmeans <- as.factor(c("A", "B", "C")[cluster_kmeans])

table(cluster_kmeans, clustersWard) #En las filas se ve el primer argumento (los clusters de K-Means)
# Podemos ver que intercambiando cambiando el orden de las columnas al de 3, 1 y 2 la logramos.
# Esto implica cambiar los valores de los clusters usados al crear la columna factor en el dataframe
comp$clustersWard <- as.factor(c("C", "A", "B")[clustersWard])
table(comp$cluster_kmeans, comp$clustersWard)

ggplot(data=comp, aes(x=Dim.1, y=Dim.2)) +
  geom_point(size=5, alpha=.2, aes(color=cluster_kmeans)) + 
  geom_point(size=2, alpha=1, aes(color=clustersWard)) +
  labs(title="K-Means (color externo) vs Ward (color interno)", color="Clusters")

```



Al comparar K-Means con Ward, vemos una gran coincidencía excepto por dos páginas web . Esto se puede explicar por la similitud entre el criterio de K-Means de minimización de las distancias internas de los clusters (WSS) y de el de Ward de minimización de la varianza.

## 3.4 Conclusiones

### 3.4.1 Análisis de componentes principales(PCA)

Del análisis de componentes principales se puede concluir que es necesario retener 4 componentes para acumular un 90% de la variabilidad, adicional a esto se observa que las variables largourl y numcarespeciales presentan un comportamiento en dirección opuesta a todas las demás variables,es decir presentan un comportamiento inverso con respecto a los demás, adicionalmente hay un conjunto de variables asociadas a las transferencias cliente-servidor que se encuentran aportando a la primera componente, en cuanto a las páginas web se puede observar que se distinguen 3 grupos diferentes en el primer plano factorial.

### 3.4.2 Cluster(k-means)

Según el resumen de los criterios de selección del K, K=3 es seleccionado por dos métodos, mientras que k=4 y k=5 son seleccionados solo por un método, por lo tanto, el número de clúster ideal sería K=3, dadas sus mejores métricas de WSS y silueta, aunque K=4 no estaría desacertado dados el Método de Calinski-Harabasz y buenos valores de clusterboot.

De la caracterización de los clusters se puede concluir que:

El cluster rojo presenta pocos caracteres en la url, pocos caracteres especiales en la url, pocos números números de parquetes DNS generados en la comunicación y grandes transferencias cliente-servidor.

El cluster verde presenta pocos caracteres en la url, pocos caracteres especiales en la url, grandes números números de parquetes DNS generados en la comunicación y valores medios en las transferencias cliente-servidor.

El cluster azul pocos caracteres en la url, pocos caracteres especiales en la url, pocos números números de parquetes DNS generados en la comunicación y pocas transferencias cliente servidor.

### 3.4.2 Cluster(jerarquico)

En el dendograma se puede observar el corte para seleccionar tres particiones en los datos, el último grupo de la derecha presenta un comportamiento un poco atipico en comparación de los demás clusters. Al comparar K-Means con Ward, vemos una gran coincidencia excepto por dos páginas web. Esto se puede explicar por la similitud entre el criterio de K-Means de minimización de las distancias internas de los clusters (WSS) y de el de Ward de minimización de la varianza. 

Como conclusión al realizar el análisis clúster se recomienda utilizar ambas metodologías ya que una complementa la otra, en este caso K-means dio excelentes resultados encontrando el K ideal, y el jerárquico presenta un buen comportamiento cuando se aplica utilizando el K encontrado, por ende se recomienda usar el clúster encontrado en el análisis clúster jerárquico ya que se distinguen mejor los grupos de manera visual en el primer plano factorial, aunque K-means no estaría desacertado dados sus buenos resultados en la selección del K.
